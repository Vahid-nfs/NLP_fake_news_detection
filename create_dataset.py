import numpy as np import pandas as pdimport networkx as nxfrom glob import globfrom collections import OrderedDictimport torchfrom sklearn.model_selection import  train_test_splitimport datetimefrom tqdm import tqdmfrom  datetime import datetimeimport pickleimport os from collections import OrderedDictfile_number=0outputs_path=f"./run{file_number}/dataset_outputs"os.makedirs(outputs_path,exist_ok=True)start_date=pd.to_datetime("2018-01-01")end_date=pd.to_datetime("2020-01-01")min_sim_value=.9df=pd.read_json("politifact_factcheck_data.json", lines=True)df=df.drop(columns=["factchecker","factcheck_date","factcheck_analysis_link"])df.statement_date=pd.to_datetime(df.statement_date)df=df[start_date <= df.statement_date][df.statement_date < end_date]df=df.sort_values("statement_date" , ignore_index=True)df["id"]=df.indexpaths=sorted(glob("./similarity_outputs/last_hidden_state/*.pt"))last_hidden_state=[]for path in paths:    temp=torch.load(path)    last_hidden_state.append(temp)last_hidden_state=torch.vstack(last_hidden_state)# df["bert_outputs"]=df["id"].apply(lambda x: pooler_output[x])# df["bert_LHS"]=df["id"].apply(lambda x: last_hidden_state[x])CosineSim = torch.nn.CosineSimilarity(dim=1)df_train,df_test=train_test_split(df,test_size=.2)#create graph and connect train nodesG=nx.Graph()G.add_nodes_from(df_train["id"].values)base_nodes=G.nodesfor node1 in tqdm(G.nodes):    for node2 in G.nodes:        if node1 != node2:            sim=CosineSim(last_hidden_state[node1],last_hidden_state[node2]).mean().item()            if sim >min_sim_value:                G.add_edge(node1,node2 ,weight=1-sim)                                # connect test nodesG.add_nodes_from(df_test["id"].values)test_nodes=df_test["id"].valuesfor node1 in tqdm(test_nodes):    for node2 in G.nodes:        sim=CosineSim(last_hidden_state[node1],last_hidden_state[node2]).mean().item()        if sim >min_sim_value:            G.add_edge(node1,node2 ,weight=1-sim)def apply_to_df(df,col_name,input_dict,target_col="id"):    df[col_name]=df[target_col].apply(lambda x : input_dict[x])    return df# degreedegree=OrderedDict(G.degree())           df_train=apply_to_df(df_train,col_name="degree",input_dict=degree)df_test=apply_to_df(df_test,col_name="degree",input_dict=degree)#degree_centralitydegree_centrality=OrderedDict(nx.degree_centrality(G))df_train=apply_to_df(df_train,col_name="degree_centrality",input_dict=degree_centrality)df_test=apply_to_df(df_test,col_name="degree_centrality",input_dict=degree_centrality)#closeness_centralitycloseness_centrality=OrderedDict(nx.closeness_centrality(G))df_train=apply_to_df(df_train,col_name="closeness_centrality",input_dict=closeness_centrality)df_test=apply_to_df(df_test,col_name="closeness_centrality",input_dict=closeness_centrality)#betweenness_centralitybetweenness_centrality=OrderedDict(nx.betweenness_centrality(G))df_train=apply_to_df(df_train,col_name="betweenness_centrality",input_dict=betweenness_centrality)df_test=apply_to_df(df_test,col_name="betweenness_centrality",input_dict=betweenness_centrality)#betweenness_centralityeigenvector_centrality=OrderedDict(nx.eigenvector_centrality(G))df_train=apply_to_df(df_train,col_name="eigenvector_centrality",input_dict=eigenvector_centrality)df_test=apply_to_df(df_test,col_name="eigenvector_centrality",input_dict=eigenvector_centrality)#clusteringclustering=OrderedDict(nx.clustering(G))df_train=apply_to_df(df_train,col_name="clustering",input_dict=clustering)df_test=apply_to_df(df_test,col_name="clustering",input_dict=clustering)                                  pickle.dump(G, open(f'{outputs_path}/graph.pickle', 'wb'))G = pickle.load(open(f'{outputs_path}/graph.pickle', 'rb'))neighbors_data=[]for ID in G.nodes():    min_weight_neighbors = sorted(G[ID].items(), key=lambda e: e[1]["weight"] )[:3]    nodes=[]    dist=[]    for item in min_weight_neighbors:        nodes.append(item[0])        dist.append(item[1]["weight"])    neighbors_data.append([ID,nodes,dist])df_train.to_csv(f"{outputs_path}/train_data.csv",index=False)df_test.to_csv(f"{outputs_path}/test_data.csv",index=False)