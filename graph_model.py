import numpy as np import pandas as pdimport networkx as nximport jsonfrom glob import globfrom collections import OrderedDictfrom torch.utils.data import DataLoader,Datasetimport torchimport torch.nn.functional as Ffrom torch import nnfrom transformers import BertTokenizer, BertModelfrom sklearn.metrics import  confusion_matrix ,accuracy_score ,recall_score ,f1_score ,classification_report ,cohen_kappa_score ,precision_scoreimport matplotlib.pyplot  as pltimport  seaborn as snsfrom tqdm import tqdmfile_number=0threshold=.9outputs_size=2batch_size=64epochs=10path=f"./run{file_number}/dataset_outputs"res_path="./run{file_number}/dataset_outputs"checkpoint_path="./run{file_number}/checkpoints"train_df=pd.read_csv(f"{path}/train_data.csv")test_df=pd.read_csv(f"{path}/test_data.csv")test_ids,train_ids=test_df.id.values,train_df.id.valuesdf=pd.concat((test_df,train_df))df=pd.get_dummies(df,columns=["statement_source"])df.statement_date=pd.to_datetime(df.statement_date)exog_cols=["statement_source_advertisement","statement_source_blog","statement_source_campaign","statement_source_email",           "statement_source_meeting","statement_source_news","statement_source_other","statement_source_radio",           "statement_source_social_media","statement_source_speech","statement_source_statement","statement_source_television",           "statement_source_testimony"]exog_input_size=len(exog_cols)df["exog"]=df.apply(lambda x : list(x[exog_cols].values) ,axis=1)mapper=OrderedDict([["true",0],                    ["mostly-true",0],                    ["half-true",0],                    ["pants-fire",1],                    ["mostly-false",1],                    ["false",1]])df["label"]=df["verdict"].apply(lambda x : mapper[x])train_df=df[df.id.isin(train_ids)]test_df=df[df.id.isin(test_ids)]class dataset(Dataset):    def __init__(self,dataframe):        super().__init__()        self.df=dataframe            def __len__(self):        return len(self.df)        def __getitem__(self,index):        txt,lbl,exog=self.df.iloc[index][["statement","label","exog"]]        lbl=torch.tensor(lbl).long()        lbl=F.one_hot(lbl,outputs_size)        exog=torch.tensor(exog).float()        return txt,exog,lbl        train_dataset=dataset(train_df)   train_loader=DataLoader(train_dataset,batch_size=batch_size)test_dataset=dataset(test_df)   test_loader=DataLoader(test_dataset,batch_size=2)        class model (nn.Module):    def __init__(self,exog=True):        super(model,self).__init__()        exog_out_size=10        self.exog=exog        self.tokenizer=BertTokenizer.from_pretrained('bert-base-uncased',padding=True)        self.bert=BertModel.from_pretrained("bert-base-uncased")        self.fc1=nn.Linear(768,256)        self.fc2=nn.Linear(256,32)        self.fc3=nn.Linear(32+exog_out_size,16)        self.exog_fc1=nn.Linear(exog_input_size,exog_out_size)        self.fc4=nn.Linear(16,outputs_size)            def forward(self,inputs,exog_input):        encoded_input=self.tokenizer(inputs, return_tensors='pt', padding=True, truncation=True)        outputs=torch.squeeze(self.bert(**encoded_input)[-1])        output=F.relu(self.fc1(outputs))        output=F.relu(self.fc2(output))         if self.exog:            exog_output=F.relu(self.exog_fc1(exog_input))            output=torch.concat((output,exog_output),axis=1)        output=F.selu(self.fc3(output))        output=torch.softmax(self.fc4(output),1)        return(output)        bert_model=model()  from transformers import AdamWoptimizer = AdamW(bert_model.parameters(),lr = 1e-5)# optimizer=torch.optim.Adam(bert_model.parameters(),lr=1e-3)        criterion=nn.CrossEntropyLoss() total_step=len(train_loader)for epoch in range(epochs):    for step,(txts,exogs,lbls) in enumerate(train_loader):        train_pred=bert_model(txts,exogs)        loss=criterion(lbls.float(),train_pred)        loss.backward()        optimizer.step()        optimizer.zero_grad()        pred=torch.argmax(train_pred,1)        lbls=torch.argmax(lbls,1)        correct = (pred == lbls).sum().item()        acc=100*(correct/len(train_pred))        if (step+1) % 1 == 0:            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f} , accuracy: {:.2f}'                     .format(epoch+1, epochs+1, step+1, total_step, loss.item(),acc))            print(confusion_matrix(lbls.detach().numpy(),pred.detach().numpy()))            torch.save(bert_model,f"{checkpoint_path}/model_{epoch}.pth")torch.save(bert_model.state_dict(), f"{checkpoint_path}/model_std_{epoch}.pt")train_loader=DataLoader(train_dataset,batch_size=2)t_pred=np.array([])t_lbls=np.array([])for txts,exogs,lbls in tqdm(train_loader):    train_pred=bert_model(txts,exogs)    train_pred=torch.argmax(train_pred,1).detach().numpy()    train_lbls=torch.argmax(lbls,1).detach().numpy()        t_pred=np.hstack((t_pred,train_pred))    t_lbls=np.hstack((t_lbls,train_lbls))train_acc=accuracy_score(t_lbls,t_pred)train_recall=recall_score(t_lbls,t_pred,average='weighted')train_f1=f1_score(t_lbls,t_pred,average='weighted')print(f"train accuracy: {train_acc}")print(f"train recall: {train_recall}")print(f"train f1_score: {train_f1}")print (classification_report(t_lbls,t_pred))    t_pred=np.array([])t_lbls=np.array([])for txts,exogs,lbls in tqdm(test_loader):    tset_pred=bert_model(txts,exogs)    test_pred=torch.argmax(tset_pred,1).detach().numpy()    test_lbls=torch.argmax(lbls,1).detach().numpy()        t_pred=np.hstack((t_pred,test_pred))    t_lbls=np.hstack((t_lbls,test_lbls))        test_acc=accuracy_score(t_lbls,t_pred)test_recall=recall_score(t_lbls,t_pred,average='weighted')test_f1=f1_score(t_lbls,t_pred,average='weighted')test_kappa=cohen_kappa_score(t_lbls,t_pred)test_precision=precision_score(t_lbls,t_pred,average='weighted')print(f"test accuracy: {test_acc}")print(f"test recall: {test_recall}")print(f"test f1_score: {test_f1}")print (classification_report(t_lbls,t_pred))res_file=open(f"{res_path}/result.txt","w+")res_file.write(f"classification report for threshold : {threshold}\n")res_file.write(f"kappa score : {test_kappa}\n")res_file.write(f"accuracy score : {test_acc}\n")res_file.write(f"recall score : {test_recall}\n")res_file.write(f"f1 score : {test_f1}\n")res_file.write(f"precision score : {test_precision}\n")res_file.write(classification_report(t_lbls,t_pred))res_file.close()res_dict=OrderedDict({"True":0,"False":1})def plot_confusion_matrix(true,pred,f_name=None):    fig,ax=plt.subplots(figsize = (9,7))    key=list(res_dict.keys())    sns.heatmap(confusion_matrix(true,pred),                yticklabels=key,                xticklabels=key,                 annot=True, fmt='d')    ax.set_xlabel('Predicted')    ax.set_ylabel('Truth')    fig.show()    if f_name:        fig.savefig(f_name,dpi=600)plot_confusion_matrix(t_lbls,t_pred,f_name=f"{res_path}/confusion_matrix.jpg")        